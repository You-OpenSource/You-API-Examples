{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# you.com News API <> Llama Index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DISCLAIMER: This notebook requires the latest version of LlamaIndex to be installed from this [branch](https://github.com/run-llama/llama_index/pull/13934).  The cells in this notebook will not successfully run if the latest version of LlamaIndex is not being utilized."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a chatbot that can provide personalized news on any topic using you.com news API involves several steps, including obtaining the necessary credentials, and programming your chatbot to use the API. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "! pip install llama-index-retrievers-you==0.1.2\n",
    "! pip install dotenv==0.0.5\n",
    "! pip install python-dotenv==1.0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load API keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# assumes a .env file exists with api keys YDC_API_KEY and OPENAI_API_KEY\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "YouRetriever.__init__() got an unexpected keyword argument 'endpoint_type'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# TODO update the pinned version of llama-index-retrievers-you after PR is merged: https://github.com/run-llama/llama_index/pull/13934\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mllama_index\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mretrievers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01myou\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m YouRetriever\n\u001b[0;32m----> 4\u001b[0m retriever \u001b[38;5;241m=\u001b[39m \u001b[43mYouRetriever\u001b[49m\u001b[43m(\u001b[49m\u001b[43mendpoint_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnews\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m retrieved_results \u001b[38;5;241m=\u001b[39m retriever\u001b[38;5;241m.\u001b[39mretrieve(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnational parks in the US\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(retrieved_results[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mget_content())\n",
      "\u001b[0;31mTypeError\u001b[0m: YouRetriever.__init__() got an unexpected keyword argument 'endpoint_type'"
     ]
    }
   ],
   "source": [
    "# TODO update the pinned version of llama-index-retrievers-you after PR is merged: https://github.com/run-llama/llama_index/pull/13934\n",
    "from llama_index.retrievers.you import YouRetriever\n",
    "\n",
    "retriever = YouRetriever(endpoint_type=\"news\")\n",
    "retrieved_results = retriever.retrieve(\"national parks in the US\")\n",
    "\n",
    "print(retrieved_results[0].get_content())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a chat engine\n",
    "\n",
    "Conceptually, it is a stateful analogy of a Query Engine. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'retriever' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mllama_index\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mchat_engine\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcontext\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ContextChatEngine\n\u001b[0;32m----> 3\u001b[0m chat_engine \u001b[38;5;241m=\u001b[39m ContextChatEngine\u001b[38;5;241m.\u001b[39mfrom_defaults(retriever\u001b[38;5;241m=\u001b[39m\u001b[43mretriever\u001b[49m)\n\u001b[1;32m      4\u001b[0m response \u001b[38;5;241m=\u001b[39m chat_engine\u001b[38;5;241m.\u001b[39mchat(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCanada\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(response)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'retriever' is not defined"
     ]
    }
   ],
   "source": [
    "from llama_index.core.chat_engine.context import ContextChatEngine\n",
    "\n",
    "chat_engine = ContextChatEngine.from_defaults(retriever=retriever)\n",
    "response = chat_engine.chat(\"Canada\")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Include a timeframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'retriever' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mllama_index\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mchat_engine\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcontext\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ContextChatEngine\n\u001b[0;32m----> 3\u001b[0m chat_engine \u001b[38;5;241m=\u001b[39m ContextChatEngine\u001b[38;5;241m.\u001b[39mfrom_defaults(retriever\u001b[38;5;241m=\u001b[39m\u001b[43mretriever\u001b[49m)\n\u001b[1;32m      4\u001b[0m response \u001b[38;5;241m=\u001b[39m chat_engine\u001b[38;5;241m.\u001b[39mchat(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNorway in Jan 2024\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(response)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'retriever' is not defined"
     ]
    }
   ],
   "source": [
    "from llama_index.core.chat_engine.context import ContextChatEngine\n",
    "\n",
    "chat_engine = ContextChatEngine.from_defaults(retriever=retriever)\n",
    "response = chat_engine.chat(\"Norway in Jan 2024\")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a chat engine with memory\n",
    "\n",
    "By keeping track of the conversation history, it can answer questions with past context in mind.\n",
    " \n",
    "`condense_plus_context` - A combination of condense_question and context. Look at the chat history and re-write the user message to be a retrieval query for the index. The retrieved text is inserted into the system prompt, so that the chat engine can either respond naturally or use the context from the query engine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'llama_index.llms'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mllama_index\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmemory\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ChatMemoryBuffer\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mllama_index\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mchat_engine\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CondensePlusContextChatEngine\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mllama_index\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mllms\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mopenai\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m OpenAI\n\u001b[1;32m      5\u001b[0m memory \u001b[38;5;241m=\u001b[39m ChatMemoryBuffer\u001b[38;5;241m.\u001b[39mfrom_defaults(token_limit\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3900\u001b[39m)\n\u001b[1;32m      6\u001b[0m llm \u001b[38;5;241m=\u001b[39m OpenAI(model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpt-4o\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'llama_index.llms'"
     ]
    }
   ],
   "source": [
    "from llama_index.core.memory import ChatMemoryBuffer\n",
    "from llama_index.core.chat_engine import CondensePlusContextChatEngine\n",
    "from llama_index.llms.openai import OpenAI\n",
    "\n",
    "memory = ChatMemoryBuffer.from_defaults(token_limit=3900)\n",
    "llm = OpenAI(model=\"gpt-4o\")\n",
    "\n",
    "chat_engine = CondensePlusContextChatEngine.from_defaults(\n",
    "    retriever=retriever,\n",
    "    memory=memory,\n",
    "    llm=llm,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'chat_engine' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# relevant news article with a summary of the news\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mchat_engine\u001b[49m\u001b[38;5;241m.\u001b[39mchat(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSalesforce\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(response)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'chat_engine' is not defined"
     ]
    }
   ],
   "source": [
    "# relevant news article with a summary of the news\n",
    "response = chat_engine.chat(\"Salesforce\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'chat_engine' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# uses memory to relate the Hubspot query to market performance news in the query above\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mchat_engine\u001b[49m\u001b[38;5;241m.\u001b[39mchat(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHubspot\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(response)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'chat_engine' is not defined"
     ]
    }
   ],
   "source": [
    "# uses memory to relate the Hubspot query to market performance news in the query above\n",
    "response = chat_engine.chat(\"Hubspot\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Personalize news\n",
    "\n",
    "Using a custom prompt, this news feed can be customized to specific threads, such as this one focused on politics and economics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'OpenAI' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 16\u001b[0m\n\u001b[1;32m      5\u001b[0m custom_prompt \u001b[38;5;241m=\u001b[39m PromptTemplate(\n\u001b[1;32m      6\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\\\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;124;03mYou're a top-tier reporter focused on news about {question}.\\n\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     13\u001b[0m )\n\u001b[1;32m     15\u001b[0m memory \u001b[38;5;241m=\u001b[39m ChatMemoryBuffer\u001b[38;5;241m.\u001b[39mfrom_defaults(token_limit\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3900\u001b[39m)\n\u001b[0;32m---> 16\u001b[0m llm \u001b[38;5;241m=\u001b[39m \u001b[43mOpenAI\u001b[49m(model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpt-4o\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     18\u001b[0m chat_engine \u001b[38;5;241m=\u001b[39m CondensePlusContextChatEngine\u001b[38;5;241m.\u001b[39mfrom_defaults(\n\u001b[1;32m     19\u001b[0m     retriever\u001b[38;5;241m=\u001b[39mretriever,\n\u001b[1;32m     20\u001b[0m     condense_question_prompt\u001b[38;5;241m=\u001b[39mcustom_prompt,\n\u001b[1;32m     21\u001b[0m     memory\u001b[38;5;241m=\u001b[39mmemory,\n\u001b[1;32m     22\u001b[0m     llm\u001b[38;5;241m=\u001b[39mllm,\n\u001b[1;32m     23\u001b[0m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'OpenAI' is not defined"
     ]
    }
   ],
   "source": [
    "from llama_index.core.memory import ChatMemoryBuffer\n",
    "from llama_index.core.chat_engine import CondensePlusContextChatEngine\n",
    "from llama_index.core import PromptTemplate\n",
    "\n",
    "custom_prompt = PromptTemplate(\n",
    "    \"\"\"\\\n",
    "You're a top-tier reporter focused on news about {question}.\\n\n",
    "YOU SHOULD ALWAYS RESEARCH THE FOLLOWING QUERIES: \\n\n",
    "1. {question} recent news and latest development\\n\n",
    "2. recent regulatory changes affecting {question}\\n\n",
    "3. recent economic events impacting {question}\\n\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "memory = ChatMemoryBuffer.from_defaults(token_limit=3900)\n",
    "llm = OpenAI(model=\"gpt-4o\")\n",
    "\n",
    "chat_engine = CondensePlusContextChatEngine.from_defaults(\n",
    "    retriever=retriever,\n",
    "    condense_question_prompt=custom_prompt,\n",
    "    memory=memory,\n",
    "    llm=llm,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'chat_engine' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mchat_engine\u001b[49m\u001b[38;5;241m.\u001b[39mchat(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMongolia\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(response)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'chat_engine' is not defined"
     ]
    }
   ],
   "source": [
    "response = chat_engine.chat(\"Mongolia\")\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
