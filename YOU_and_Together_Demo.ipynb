{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2pp4KYyMIaIb"
      },
      "source": [
        "# Using Together Inference REST API\n",
        "\n",
        "This is the simplest way to use Together Inference using REST API. You only need to provide your Together API key and YOU API key."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gjDNSnqkNUNh"
      },
      "source": [
        "### Provide the API keys and define functions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "! pip -q install requests"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "K0G94X7L6KoH"
      },
      "outputs": [],
      "source": [
        "YOU_API_KEY = \"\" # Provide your YOU API key.\n",
        "TOGETHER_API_KEY = \"\" # Provide your TOGETHER API key.\n",
        "\n",
        "import requests\n",
        "\n",
        "def get_ai_snippets_for_query(query):\n",
        "  headers = {\"X-API-Key\": YOU_API_KEY}\n",
        "  results = requests.get(\n",
        "      f\"https://api.ydc-index.io/search?query={query}\",\n",
        "      headers=headers,\n",
        "  ).json()\n",
        "\n",
        "  # We return many text snippets for each search hit so\n",
        "  # we need to explode both levels\n",
        "  return \"\\n\".join([\"\\n\".join(hit[\"snippets\"]) for hit in results[\"hits\"]])\n",
        "\n",
        "def get_together_prompt(query, context):\n",
        "  return f\"\"\"Provide an answer based on the given context.\\n\n",
        "    Context: {context}\\n\n",
        "    Question: {query}\"\"\"\n",
        "\n",
        "def ask_together(query, context, model_api_string):\n",
        "  \"\"\"\n",
        "  Generate a response from the given query and context.\n",
        "\n",
        "  Args:\n",
        "    query: (str) your query.\n",
        "    context: (str) your context from snippets.\n",
        "    model_api_string: (str) a model API string from Together Inference. See the full list in (https://docs.together.ai/docs/inference-models)\n",
        "  \"\"\"\n",
        "  # This is hard coded here. To automatically find the default values, use the\n",
        "  # Python library as shown in the next section.\n",
        "  prompt_format = \"[INST]\\n {prompt} \\n[/INST]\\n\\n\"\n",
        "  stop_sequences =  ['[INST]', '\\n\\n']\n",
        "  max_context_length = 4096\n",
        "\n",
        "  # Truncate the context based on the model context length. Instead of using its\n",
        "  # tokenizer and the exact token count, we assume 1 token ~= ¾ words.\n",
        "  truncated_context = \" \".join(context.split(\" \")[:int(max_context_length*3/4)])\n",
        "  prompt = get_together_prompt(query, truncated_context)\n",
        "\n",
        "  # Formatting the prompt properly through the model info.\n",
        "  if prompt_format:\n",
        "    prompt_format_list = prompt_format.split(\" \")\n",
        "    formated_prompt = f\"{prompt_format_list[0]}{prompt}{prompt_format_list[2]}\"\n",
        "  else:\n",
        "    formated_prompt = prompt\n",
        "\n",
        "\n",
        "  url = \"https://api.together.xyz/inference\"\n",
        "\n",
        "  payload = {\n",
        "      \"model\": model_api_string,\n",
        "      \"prompt\": formated_prompt, # Use the corrrect prompt format.\n",
        "      \"max_tokens\": 256,\n",
        "      \"stop\":stop_sequences,\n",
        "      \"temperature\": 1.0,\n",
        "      \"top_p\": 0.7,\n",
        "      \"top_k\": 50,\n",
        "      \"repetition_penalty\": 1.1\n",
        "  }\n",
        "  headers = {\n",
        "      \"accept\": \"application/json\",\n",
        "      \"content-type\": \"application/json\",\n",
        "      \"Authorization\": f\"Bearer {TOGETHER_API_KEY}\"\n",
        "  }\n",
        "\n",
        "  response = requests.post(url, json=payload, headers=headers)\n",
        "\n",
        "  if response.status_code != 200:\n",
        "      raise ValueError(f\"Request failed with status code {response.status_code}: {response.text}\")\n",
        "\n",
        "  return response.json()['output']['choices'][0]['text']\n",
        "\n",
        "def ask_together_with_ai_snippets(query, model_api_string):\n",
        "    ai_snippets = get_ai_snippets_for_query(query)\n",
        "    return ask_together(query, ai_snippets, model_api_string)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R3rrMAD3NYoQ"
      },
      "source": [
        "### Send the query with a choice of your generation model from Together Inference API\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P1biFE3r871m",
        "outputId": "395d5617-3ee4-47ca-ee58-ab02ab0a24db"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1. Linux\n",
            "2. Git\n",
            "3. Node.js\n",
            "4. Android\n",
            "5. Ruby on Rails\n",
            "6. Python\n",
            "7. MariaDB\n",
            "8. PostgreSQL\n",
            "9. Docker\n",
            "10. Visual Studio Code (Documentation)\n"
          ]
        }
      ],
      "source": [
        "query = \"What are top 10 successful open source projects?\"\n",
        "model_api_string=\"togethercomputer/Llama-2-7B-32K-Instruct\"\n",
        "print(ask_together_with_ai_snippets(query, model_api_string))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AfGpkpJ8DCkz"
      },
      "source": [
        "# Using Together Inference Python Library\n",
        "\n",
        "This is more useful when you want to find the default prompt format and stop sequences. You need to install the together library."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L_f4tGg7Ng-6"
      },
      "source": [
        "### Provide the API keys and define functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "mVqW5EeG9Uqb"
      },
      "outputs": [],
      "source": [
        "! pip install -q together"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "O49UXxdN9YAw"
      },
      "outputs": [],
      "source": [
        "import together\n",
        "import requests\n",
        "\n",
        "YOU_API_KEY = \"\" # Provide your YOU API key.\n",
        "TOGETHER_API_KEY = \"\" # Provide your TOGETHER API key.\n",
        "\n",
        "together.api_key = TOGETHER_API_KEY\n",
        "\n",
        "def get_ai_snippets_for_query(query):\n",
        "  headers = {\"X-API-Key\": YOU_API_KEY}\n",
        "  results = requests.get(\n",
        "      f\"https://api.ydc-index.io/search?query={query}\",\n",
        "      headers=headers,\n",
        "  ).json()\n",
        "\n",
        "  # We return many text snippets for each search hit so\n",
        "  # we need to explode both levels\n",
        "  return \"\\n\".join([\"\\n\".join(hit[\"snippets\"]) for hit in results[\"hits\"]])\n",
        "\n",
        "def get_together_prompt(query, context):\n",
        "  return f\"\"\"You are an helpful assistant answering to a question based on\n",
        "    provided context. Here is a context found on the internet: {context}.\\n\n",
        "    Answer the following question: {query}\\n\"\"\"\n",
        "\n",
        "def get_model_config(model_api_string):\n",
        "  model_list = together.Models.list()\n",
        "\n",
        "  prompt_format, stop_sequences = None, []\n",
        "  context_length = 2048\n",
        "  for m in model_list:\n",
        "    if m['name'] == model_api_string:\n",
        "      if 'prompt_format' in m['config']: prompt_format = m['config']['prompt_format']\n",
        "      if 'stop' in m['config']: stop_sequences = m['config']['stop']\n",
        "      if 'context_length' in m: context_length = m['context_length']\n",
        "      break\n",
        "\n",
        "  return prompt_format, stop_sequences, context_length\n",
        "\n",
        "def ask_together(query, context, model_api_string):\n",
        "  \"\"\"\n",
        "  Generate a response from the given query and context.\n",
        "\n",
        "  Args:\n",
        "    query: (str) your query.\n",
        "    context: (str) your context from snippets.\n",
        "    model_api_string: (str) a model API string from Together Inference. See the full list in (https://docs.together.ai/docs/inference-models)\n",
        "  \"\"\"\n",
        "  prompt_format, stop_sequences, max_context_length = get_model_config(model_api_string)\n",
        "\n",
        "  # Truncate the context based on the model context length. Instead of using its\n",
        "  # tokenizer and the exact token count, we assume 1 token ~= ¾ words.\n",
        "  truncated_context = \" \".join(context.split(\" \")[:int(max_context_length*3/4)])\n",
        "  prompt = get_together_prompt(query, truncated_context)\n",
        "\n",
        "  # Formatting the prompt properly through the model info.\n",
        "  if prompt_format:\n",
        "    prompt_format_list = prompt_format.split(\" \")\n",
        "    formated_prompt = f\"{prompt_format_list[0]}{prompt}{prompt_format_list[2]}\"\n",
        "  else:\n",
        "    formated_prompt = prompt\n",
        "\n",
        "  response = together.Complete.create(\n",
        "      prompt=formated_prompt,\n",
        "      model=model_api_string,\n",
        "      max_tokens = 256,\n",
        "      temperature = 1.0,\n",
        "      top_k = 60,\n",
        "      top_p = 0.6,\n",
        "      repetition_penalty = 1.1,\n",
        "      stop = stop_sequences,\n",
        "      )\n",
        "\n",
        "  return response[\"output\"][\"choices\"][0][\"text\"]\n",
        "\n",
        "def ask_together_with_ai_snippets(query, model_api_string):\n",
        "    ai_snippets = get_ai_snippets_for_query(query)\n",
        "    return ask_together(query, ai_snippets, model_api_string)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VxIZB2_MNl3Z"
      },
      "source": [
        "### Send the query with a choice of your generation model from Together Inference API"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aLcPpwmr9vgz",
        "outputId": "ef287434-dbce-444c-b192-2af3612a3f92"
      },
      "outputs": [
        {
          "ename": "AttributeError",
          "evalue": "module 'together' has no attribute 'Models'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[21], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m query \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhat are top 10 successful open source projects?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      2\u001b[0m model_api_string\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtogethercomputer/Llama-2-7B-32K-Instruct\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mask_together_with_ai_snippets\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_api_string\u001b[49m\u001b[43m)\u001b[49m)\n",
            "Cell \u001b[0;32mIn[20], line 77\u001b[0m, in \u001b[0;36mask_together_with_ai_snippets\u001b[0;34m(query, model_api_string)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mask_together_with_ai_snippets\u001b[39m(query, model_api_string):\n\u001b[1;32m     76\u001b[0m     ai_snippets \u001b[38;5;241m=\u001b[39m get_ai_snippets_for_query(query)\n\u001b[0;32m---> 77\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mask_together\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mai_snippets\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_api_string\u001b[49m\u001b[43m)\u001b[49m\n",
            "Cell \u001b[0;32mIn[20], line 48\u001b[0m, in \u001b[0;36mask_together\u001b[0;34m(query, context, model_api_string)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mask_together\u001b[39m(query, context, model_api_string):\n\u001b[1;32m     40\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;124;03m  Generate a response from the given query and context.\u001b[39;00m\n\u001b[1;32m     42\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;124;03m    model_api_string: (str) a model API string from Together Inference. See the full list in (https://docs.together.ai/docs/inference-models)\u001b[39;00m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[0;32m---> 48\u001b[0m   prompt_format, stop_sequences, max_context_length \u001b[38;5;241m=\u001b[39m \u001b[43mget_model_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_api_string\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     50\u001b[0m   \u001b[38;5;66;03m# Truncate the context based on the model context length. Instead of using its\u001b[39;00m\n\u001b[1;32m     51\u001b[0m   \u001b[38;5;66;03m# tokenizer and the exact token count, we assume 1 token ~= ¾ words.\u001b[39;00m\n\u001b[1;32m     52\u001b[0m   truncated_context \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(context\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m)[:\u001b[38;5;28mint\u001b[39m(max_context_length\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m3\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m4\u001b[39m)])\n",
            "Cell \u001b[0;32mIn[20], line 26\u001b[0m, in \u001b[0;36mget_model_config\u001b[0;34m(model_api_string)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_model_config\u001b[39m(model_api_string):\n\u001b[0;32m---> 26\u001b[0m   model_list \u001b[38;5;241m=\u001b[39m \u001b[43mtogether\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mModels\u001b[49m\u001b[38;5;241m.\u001b[39mlist()\n\u001b[1;32m     28\u001b[0m   prompt_format, stop_sequences \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, []\n\u001b[1;32m     29\u001b[0m   context_length \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2048\u001b[39m\n",
            "\u001b[0;31mAttributeError\u001b[0m: module 'together' has no attribute 'Models'"
          ]
        }
      ],
      "source": [
        "query = \"What are top 10 successful open source projects?\"\n",
        "model_api_string=\"togethercomputer/Llama-2-7B-32K-Instruct\"\n",
        "print(ask_together_with_ai_snippets(query, model_api_string))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
