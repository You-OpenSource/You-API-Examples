{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e1a49484-f9e8-4a3f-8fbe-7c7032c490a8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "!pip install -q --upgrade transformers openai sentence-transformers datasets langchain==0.0.310 google-api-python-client>=2.100.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "611b289b-1f45-4f85-a630-723fc476070c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from langchain.retrievers.you import YouRetriever\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "\n",
    "os.environ[\"YDC_API_KEY\"] = \"YOUR YOU.COM API KEY\"\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"YOUR OPENAI API KEY\"\n",
    "yr = YouRetriever()\n",
    "model = \"gpt-3.5-turbo-16k\"\n",
    "qa = RetrievalQA.from_chain_type(llm=ChatOpenAI(model=model), chain_type=\"map_reduce\", retriever=yr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "00acf693-cf3d-4394-bf88-0b984153f3ab",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "\n",
    "ds = load_dataset(\"hotpot_qa\", \"fullwiki\")[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1231c8fd-d200-434e-9340-91b47657261d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from langchain.utilities import GoogleSearchAPIWrapper\n",
    "\n",
    "\n",
    "os.environ[\"GOOGLE_CSE_ID\"] = \"03879df4495b8ecfd\"\n",
    "os.environ[\"GOOGLE_API_KEY\"] = \"AIzaSyCYYNCl-jwpfXBI8zfCe5pDMJ9viviR6wo=\"\n",
    "search = GoogleSearchAPIWrapper()\n",
    "\n",
    "def top10_results(query):\n",
    "    return search.results(query, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6df592b2-4508-4399-82ff-aae91fd2080e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from langchain.schema.retriever import BaseRetriever, Document\n",
    "from typing import TYPE_CHECKING, Any, Dict, List, Optional \n",
    "from langchain.callbacks.manager import CallbackManagerForRetrieverRun, AsyncCallbackManagerForRetrieverRun\n",
    "\n",
    "\n",
    "class GoogleRetriever(BaseRetriever):\n",
    "    def __int__(self):\n",
    "        pass\n",
    "\n",
    "    def _get_relevant_documents(\n",
    "            self, query: str, *, run_manager: CallbackManagerForRetrieverRun\n",
    "    ) -> List[Document]:\n",
    "        return [Document(page_content=result.get(\"snippet\", \"\")) for result in top10_results(query)]\n",
    "\n",
    "    async def _aget_relevant_documents(\n",
    "            self,\n",
    "            query: str,\n",
    "            *,\n",
    "            run_manager: AsyncCallbackManagerForRetrieverRun,\n",
    "            **kwargs: Any,\n",
    "    ) -> List[Document]:\n",
    "        raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9b4e0cc3-13fa-4453-8778-6d9f5107ce6e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "goog_qa = RetrievalQA.from_chain_type(\n",
    "    llm=ChatOpenAI(model=model), chain_type=\"map_reduce\", retriever=GoogleRetriever()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "707ebbcd-76da-4b35-862e-529c2f76cb93",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "SAMPLE_SIZE = 100\n",
    "pds = ds.to_pandas()\n",
    "pds_sample = pds.sample(SAMPLE_SIZE).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9c303236-dca8-4fdb-b1ae-378629541ce9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def parallel_progress_apply(column, callback, num_workers):\n",
    "    with ThreadPoolExecutor(max_workers=num_workers) as executor:\n",
    "        return list(tqdm(executor.map(callback, column), total=len(column)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aed5bc7a-4ec2-4831-a222-8e43d0f9244e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def get_run_chain_function(chain):\n",
    "    def run_chain(example):\n",
    "        try:\n",
    "            return chain(example)[\"result\"]\n",
    "        except:\n",
    "            return \"\"\n",
    "    return run_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "01d09a6e-39b7-4739-94a8-fefd57eb7b32",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pds_sample[\"ydc_prediction\"] = parallel_progress_apply(\n",
    "    pds_sample[\"question\"], lambda x: get_run_chain_function(qa)(x), num_workers=8\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fd32e6bd-6cb0-4bc4-b1da-3dc607d6d206",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Can't use parallel calls here because Google API so slow :/\n",
    "pds_sample[\"google_prediction\"] = pds_sample[\"question\"].apply(get_run_chain_function(goog_qa))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "67dc6256-956d-4400-b0af-47ea2c280dab",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "# This is all ripped from hotpot_qa source code with minor modifications to only return the f1 instead of the (P,R,F1) tuple\n",
    "# https://github.com/hotpotqa/hotpot/blob/master/hotpot_evaluate_v1.py#L26\n",
    "def calculate_f1_score(prediction, ground_truth):\n",
    "    normalized_prediction = normalize_answer(prediction)\n",
    "    normalized_ground_truth = normalize_answer(ground_truth)\n",
    "\n",
    "    ZERO_METRIC = (0, 0, 0)\n",
    "\n",
    "    if (\n",
    "        normalized_prediction in [\"yes\", \"no\", \"noanswer\"]\n",
    "        and normalized_prediction != normalized_ground_truth\n",
    "    ):\n",
    "        return 0\n",
    "    if (\n",
    "        normalized_ground_truth in [\"yes\", \"no\", \"noanswer\"]\n",
    "        and normalized_prediction != normalized_ground_truth\n",
    "    ):\n",
    "        return 0\n",
    "\n",
    "    prediction_tokens = normalized_prediction.split()\n",
    "    ground_truth_tokens = normalized_ground_truth.split()\n",
    "    common = Counter(prediction_tokens) & Counter(ground_truth_tokens)\n",
    "    num_same = sum(common.values())\n",
    "    if num_same == 0:\n",
    "        return 0\n",
    "    precision = 1.0 * num_same / len(prediction_tokens)\n",
    "    recall = 1.0 * num_same / len(ground_truth_tokens)\n",
    "    f1 = (2 * precision * recall) / (precision + recall)\n",
    "    return f1\n",
    "\n",
    "\n",
    "def normalize_answer(s):\n",
    "    def remove_articles(text):\n",
    "        return re.sub(r\"\\b(a|an|the)\\b\", \" \", text)\n",
    "\n",
    "    def white_space_fix(text):\n",
    "        return \" \".join(text.split())\n",
    "\n",
    "    def remove_punc(text):\n",
    "        exclude = set(string.punctuation)\n",
    "        return \"\".join(ch for ch in text if ch not in exclude)\n",
    "\n",
    "    def lower(text):\n",
    "        return text.lower()\n",
    "\n",
    "    return white_space_fix(remove_articles(remove_punc(lower(s))))\n",
    "\n",
    "\n",
    "def exact_match_score(prediction, ground_truth):\n",
    "    return normalize_answer(prediction) == normalize_answer(ground_truth)\n",
    "\n",
    "\n",
    "def filter_wiki_citation(snip):\n",
    "    return not snip.startswith(\"- ^\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d406dbfd-c187-4c82-b3d6-3b8b4726c9e5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pds_sample[\"ydc_f1\"] = parallel_progress_apply(\n",
    "    list(pds_sample.iterrows()),\n",
    "    lambda x: calculate_f1_score(x[1][\"ydc_prediction\"], x[1][\"answer\"]),\n",
    "    num_workers=8,\n",
    ")\n",
    "pds_sample[\"google_f1\"] = parallel_progress_apply(\n",
    "    list(pds_sample.iterrows()),\n",
    "    lambda x: calculate_f1_score(x[1][\"google_prediction\"], x[1][\"answer\"]),\n",
    "    num_workers=8,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9734c64e-3514-45c7-acba-ce15e45571c9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"YDC F1\")\n",
    "print(pds_sample[\"ydc_f1\"].mean())\n",
    "print(\"Google F1\")\n",
    "print(pds_sample[\"google_f1\"].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a9ef0926-d8a6-4593-ac12-d6af67b22e70",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Langchain Retriever Evals",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
